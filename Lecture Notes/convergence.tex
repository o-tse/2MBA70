  

\section{Convergence Theorems}

One of the motivations for developing a new theory of integration using measurable functions instead of continuous ones, was that we would be able to change limits and integrals more often. We have already seen an example of such a result in the monotone convergence theorem, Theorem~\ref{th:monotone-convergence-I}. However, this required that the sequence $f_n$ of function was monotone (i.e. non-decreasing) everywhere, which sounds a bit restrictive. That is why in this section we will use the monotone convergence theorem to derive other convergence results that have less restrictive conditions.

\subsection{Monotone convergence (continued)}

Theorem~\ref{th:monotone-convergence-I} states that if we have a sequence of measurable functions $(f_n)_{n \in \bbN}$ from some measure space $(\Omega, \cF, \mu)$ to $[0,+\infty]$ such that $f_n \le f_{n+1}$, then we could interchange limits and integration so that
\[
	\lim_{n \to \infty} \int_\Omega f_n \, \dd \mu = \int_\Omega \lim_{n \to \infty} f_n \, \dd \mu.
\]
It should be noted that the monotone properties requires that $f_n(\omega) \le f_{n+1}(\omega)$ for all $\omega \in \Omega$. However, from the definition of the Lebesgue integral we see that it is not affected by sets measure zero. Hence, we would expect that we can relax the monotone property to hold $\mu$-almost everywhere, i.e. the set where it does not hold has measure zero. This turns out to be the case, providing a slightly more general version of the monotone convergence theorem.

\begin{theorem}[Monotone convergence II]\label{thm:monotone_convergence_ii}
Let $(\Omega, \cF, \mu)$ be a measure space. Let $(f_n)_{n \ge 1}$ be a sequence of non-negative, measurable functions and let $f$ be a non-negative measurable functions such that the following holds $\mu$-almost everywhere
\begin{enumerate}
\item $f_n \le f_{n+1}$ for all $n \in \bbN$, and
\item $\lim_{n \to \infty} f_n = f$.
\end{enumerate}
Then
\[
	\lim_{n \to \infty} \int_\Omega f_n \dd \mu = \int_\Omega f \dd \mu.
\]
\end{theorem}

\begin{proof}
As you might have expected, the proof will use the first monotone convergence theorem. For this we first note that by assumption there exists a $N \in \cF$ with $\mu(N) = 0$ such that properties 1 and 2 from theorem statement hold for all $\omega \in \Omega \setminus N$. Now define the function $g_n(\omega) = \max_{1 \le k \le n} f_k(\omega)$. Then $g_n(\omega) \le g_{n+1}(\omega)$ holds for \emph{all} $\omega \in \Omega$. We further define $g(\omega) := \lim_{n \to \infty} g_n(\omega)$. Here comes the key observation. For every $\omega \in \Omega\setminus N$ it holds that $g_n(\omega) = f_n(\omega)$ and $g(\omega) = f(\omega)$. Moreover, since $\mu(N) = 0$ we have that
\[
	\int_\Omega f_n \, \dd \mu = \int_\Omega g_n \, \dd \mu \quad \text{and} \quad
	\int_\Omega f \, \dd \mu = \int_\Omega g \, \dd \mu.
\]
The result then follows by applying Theorem~\ref{th:monotone-convergence-I} to the functions $g_n$ and $g$.
\end{proof}

\subsection{Fatou's Lemma}

\begin{theorem}[Fatou's lemma]\label{thm:fatou}
Let $(\Omega, \cF, \mu)$ be a measure space. Let $(f_n)_{n \ge 1}$ be a sequence of non-negative, measurable functions and define
\[
	f := \liminf_{n\to \infty} f_n = \lim_{n \to \infty} \inf_{k \ge n} f_k.
\]
Then
\[
	\int_\Omega f \dd \mu \le \liminf_{n\to \infty} \int_\Omega f_n \dd \mu
\]
\end{theorem}

\begin{proof}
Our proof will use the monotone convergence theorem. There are however other proofs, based on first principles. See for example [REF].

Define the function $g_n(\omega) := \inf_{k \ge n} f_k(\omega)$ and note that by Lemma~\ref{lem:limit_operations_measurable_functions} $g_n$ are measurable. Moreover, $g_n(\omega) \le g_{n+1}(\omega)$ for all $\omega \in \Omega$ and $\lim_{n \to \infty} g_n(\omega) = f(\omega)$. Hence, Theorem~\ref{thm:monotone_convergence_ii} implies that
\[
	\lim_{n \to \infty} \int_\Omega g_n \, \dd \mu = \int_\Omega \lim_{n \to \infty} g_n \, \dd \mu
	= \int f \, \dd \mu. 
\]
Finally we observe that by definition $g_n \le f_n$ holds for all $n \in \bbN$ so that
\begin{align*}
	\int f \, \dd \mu &= \lim_{n \to \infty} \int_\Omega g_n \, \dd \mu \\
	&= \lim_{n \to \infty} \int_\Omega \inf_{k \ge n} f_k \, \dd \mu \\
	&\le \lim_{n \to \infty} \inf_{\ell \ge n} \int_\Omega  f_\ell \, \dd \mu\\
	&= \liminf_{n \to \infty} \int_\Omega  f_\ell \, \dd \mu.
\end{align*}
Here we used that $\inf_{k \ge n} f_k \le f_\ell$ for all $\ell \ge k$ and monotonicity of the integral (see Proposition~\ref{prop:properties-integral}).
\end{proof}

\subsection{Dominated Convergence}

Armed with Fatou's lemma we can now prove one of the most useful convergence results for Lebesgue integrals.

\begin{theorem}[Dominated convergence]\label{thm:dominated_convergence}
Let $(\Omega, \cF, \mu)$ be a measure space. Let $(f_n)_{n \ge 1}$ be a sequence of non-negative, measurable functions and let $f$ be a non-negative measurable functions such that $f_n \to f$ point-wise $\mu$-almost everywhere. Moreover, assume there exists a non-negative $\mu$-integrable function $g : \Omega \to [0,\infty]$ such that $|f_n| \le g$ $\mu$-almost everywhere. Then
\[
	\lim_{n \to \infty} \int_\Omega f_n \dd \mu = \int_\Omega f \dd \mu.
\]
\end{theorem}

\begin{proof}
We will first proof the result for the case that both $|f_n| \le g$ and $f_n \to f$ hold everywhere.  

Consider the functions $f_n+g$, and note that $|f_n| \le g$ implies that these are non-negative. Fatou's lemma (Theorem~\ref{thm:fatou}) now implies that
\[
	\int_\Omega f + g \, \dd \mu \le \liminf_{n \to \infty} \int_\Omega f_n + g \, \dd \mu.
\] 
Using the additive property ot the integral we get
\[
	\int_\Omega f \, \dd \mu + \int_\Omega g \, \dd \mu 
	\le \liminf_{n \to \infty} \int_\Omega f_n \, \dd \mu + \int_\Omega g \, \dd \mu.
\]
Since $\int_\Omega g \, \dd \mu < \infty$ this implies that
\[
	\int_\Omega f \, \dd \mu \le \liminf_{n \to \infty} \int_\Omega f_n \, \dd \mu.
\]
On the other hand, the condition $|f_n| \le g$ also implies that the functions $g - f_n$ are non-negative. Applying Fatou's lemma here yields
\[
	\int_\Omega g - f \, \dd \mu \le \liminf_{n \to \infty} \int_\Omega g - f_n \, \dd \mu.
\]
The additive property of integral now yields
\[
	\int_\Omega g \dd \mu - \int_\Omega f \, \dd \mu \le \int_\Omega g \, \dd \mu 
	+ \liminf_{n \to \infty} \int_\Omega - f_n \, \dd \mu,
\]
which implies that 
\[
	\int_\Omega f \, \dd \mu \ge - \liminf_{n \to \infty} \int_\Omega - f_n \, \dd \mu 
	= \limsup_{n \to \infty} \int_\Omega f_n \, \dd \mu.
\]
We thus conclude that
\[
	\lim_{n \to \infty} \int_\Omega f_n \, \dd \mu = \int_\Omega f \, \dd \mu.
\]

Now let us consider the general case. Then there exists a $N \in \cF$ such that $\mu(N) = 0$ and both $|f_n| \le g$ and $f_n \to f$ hold for every $\omega \in \Omega \setminus N$. Let us now define the following functions
\[
	\hat{f}_n(\omega) = \begin{cases}
		f_n(\omega) &\text{if } \omega \in \Omega \setminus N,\\
		0 &\text{else,}
	\end{cases}
	\quad 
	\hat{f}(\omega) = \begin{cases}
		f(\omega) &\text{if } \omega \in \Omega \setminus N,\\
		0 &\text{else,}
	\end{cases}
\]
and
\[
	\hat{g}(\omega) = \begin{cases}
		g(\omega) &\text{if } \omega \in \Omega \setminus N,\\
		0 &\text{else.}
	\end{cases}
\]

Then 
\[
	\int_\Omega \hat{f}_n \, \dd \mu = \int_\Omega f_n \, \dd \mu \quad \text{and} \quad
	\int_\Omega \hat{f} \, \dd \mu = \int_\Omega f \, \dd \mu
\]
Moreover, $\hat{f}_n \le \hat{g}$ and $\hat{f}_n \to \hat{f}$ hold \emph{everywhere}. So using the first part of the proof we have that
\[
	\lim_{n \to \infty} \int_\Omega f_n \, \dd \mu = \lim_{n \to \infty} \int_\Omega \hat{f}_n \, \dd \mu
	= \lim_{n \to \infty} \int_\Omega \hat{f} \, \dd \mu = \lim_{n \to \infty} \int_\Omega f \, \dd \mu.
\]
\end{proof}

\begin{example}
Consider the sequence of functions $f_n(x) = \frac{n \sin(x/n)}{x(x^2+1)}$. We will use dominated convergence to determine $\lim_{n \to \infty} \int_\bbR f_n \, \dd \lambda$. Define $g(x) = \frac{1}{x^2 +1}$ and note that 
\[
	f_n(x) = \frac{\sin(x/n)}{x/n} g(x).
\]
Note that $|\sin(y)| \le |y|$ holds for all $y > 0$ and that for every $x$ we have that $\lim_{n \to \infty} \frac{\sin(x/n)}{x/n} = 1$. We thus conclude that $|f_n(x)| \le g(x)$ and $f_n \to g(x)$ holds for all $x \in \bbR \setminus \{0\}$. Since the set $\{0\}$ has Lebesgue measure zero, all the conditions of Theorem~\ref{thm:dominated_convergence} are satisfied. Hence (see Example~\ref{ex:computation_lebesgue_integral})
\[
	\lim_{n \to \infty} \int_\bbR \frac{n \sin(x/n)}{x(x^2+1)} \lambda(\dd x) 
	= \int_\bbR \frac{1}{x^2 +1} \lambda(\dd x) = \pi.
\]
\end{example}

\section{Convergence of random variables}

Consider a sequence $(X_n)_{n \ge 1}$ of random variables. Similar to the setting of real number, we would like to have a notion of convergence of this sequence. In other words we would like to say that $X_n \to X$ where $X$ is a different random variable. It turns out that there is different ways to define the concept of convergence of random variables. In this section we will discuss three of them: convergence in distribution, convergence in probability and almost sure convergence. While the last one has a more straightforward definition (see ??) the other two rely on a notion of convergence of probability measures, which we will discuss first.

\subsection{Weak convergence of probability measures}

Let $(\Omega, \cF, \mu)$ be a measure space and $f : \Omega \to \bbR$ be $\mu$-integrable. Then we define the measure $\mu \ast f : \cF \to [0,\infty]$ as
\begin{equation}
	\mu \ast f (A) := \int_A f \dd \mu \quad \text{for } A \in \cF.
\end{equation}

\begin{definition}
Let $(\mu_n)_{n \ge 1}$ and $\mu$ be probability measures on $(\bbR, \cB_\bbR)$. We say that $\mu_n$ \emph{converges weakly} to $\mu$ if for every continuous bounded function $f : \bbR \to \bbR$ it holds that
\[
	\mu_n \ast f \to \mu \ast f.
\]
If this is the case we write $\mu_n \Rightarrow \mu$.
\end{definition}

The definition of weak convergence ask us to verify the convergence of the $\mu_n$ integral of $h$ for any continuous bounded function $h$. In some cases that can be cumbersome task. Hence it would be helpful if we would have some equivalent conditions for weak convergence. The beauty here is that there are many equivalent definitions. They are often summarized in what is known as the Portmanteau lemma (or theorem). We provide one version of it below.

We will first prove an important technical lemma, needed for the proof of this theorem. For any function $h : \bbR \to \bbR$ we denote by $\cC_h \subset \bbR$ the set of continuity points of $h$, i.e. the set of all points $x\in \bbR$ at which $h$ is continuous. 

The following technical lemma allows us to approximates measurable functions by continuous ones, with arbitrary precision in terms of the integrals.

\begin{lemma}
Let $\mu$ be a probability measure on $(\bbR, \cB_\bbR)$ and $h : \bbR \to \bbR$ be a bounded measurable function with $\mu(\cC_h) = 1$. Then for every $\varepsilon > 0$, there exist continuous bounded functions $h^-_\varepsilon$ and $h^+_\varepsilon$ such that
\begin{enumerate}
\item $h^-_\varepsilon \le h \le h^+_\varepsilon$ and
\item $\mu \ast h^+_\varepsilon - \mu \ast h^-_\varepsilon < \varepsilon$.
\end{enumerate} 
\end{lemma}

\begin{proof}
TODO
\end{proof}

Recall that a set $A \subset \bbR^d$ is open if for every $x \in A$ there exists an $r > 0$ such that $B_x(r) \subset A$. In addition, a set $B\subset \bbR^d$ is called \emph{closed} if $B = \bbR^d \setminus A$ for some open set $A$.

For a set $A \subset \bbR$ denote by $\bar{A}$ the smallest closed set that contains $A$ and by $A^\circ$ the largest open set that is contained in $A$. The sets $\bar{A}$ and $A^\circ$ are called the \emph{closure} and \emph{interior} of $A$, respectively. We now define the \emph{boundary} of $A$ as $\partial A := \bar{A} \setminus A^\circ$. Given a measure $\mu$ on $(\bbR, \cB_\bbR)$, a set $A$ is called a \emph{$\mu$-continuity set} if $\mu(\delta A) = 0$.

\pagebreak

We can now state a list of equivalent definition for weak convergence of probability measures.

\begin{theorem}[Portmanteau Theorem]\label{thm:portmanteau}
Let $(\mu_n)_{n \ge 1}$ and $\mu$ be probability measures on $(\bbR, \cB_\bbR)$. Then the following statements are equivalent:
\begin{enumerate}
\item $\mu_n \Rightarrow \mu$.
\item $\mu_n \ast h \to \mu \ast h$ for all bounded measurable functions $h: \bbR \to \bbR$ with $\mu(\cC_h) = 1$.
\item $\limsup_{n \to \infty} \mu_n(B) \le \mu(B)$ for all closed sets $B \subset \bbR$.
\item $\liminf_{n \to \infty} \mu_n(A) \ge \mu(A)$ for all open sets $A \subset \bbR$.
\item $\lim_{n \to \infty} \mu(C) = \mu(C)$ for all $\mu$-continuity sets $C$.
\end{enumerate}
\end{theorem} 

\begin{proof}
We will prove the following implication chain: 4 $\iff$ 3 $\Rightarrow$ 1 $\Rightarrow$ 2 $\Rightarrow$ 5 $\Rightarrow$ 3  

\textbf{4 $\iff$ 3:} This follows directly since every closed set $B$ is the complement of an open set $A$, i.e. $B = \bbR \setminus A$ and thus
\[
	\limsup_{n \to \infty} \mu_n(B) = \limsup_{n \to \infty} 1 - \mu_n(A) = 1 - \liminf_{n \to \infty} \mu_n(A).
\]

\textbf{3 $\Rightarrow$ 1:} Let $h$ be a continuous bounded function. Then, without loss of generality we may assume that $0 \ge h < 1$. Now fix some $k \in \bbN$ and define the following sets:
\[
	B_j := \{x \in \bbR \, : \, \frac{j}{k} \le f(x)\} \quad \text{for } j = 0, 1, \dots, k.
\]
Note that since $f$ is continuous these are closed sets. Also note that $\mu(B_0) = 1$ and $\mu(B_k) = 0$.

We further observe that $f(x) = \sum_{j = 1}^k f(x) \mathbf{1}_{B_{j-1} \cap B_j^c}$, where $B_j^c = \bbR \setminus B_j$. Hence we can now bound the integral $\mu \ast h$ from above and below as follows:
\begin{equation}\label{eq:portmanteau_1}
	\sum_{j = 1}^k \frac{j-1}{k} \mu(B_{j - 1} \cap B_j^c) \le \int_\bbR h \, \dd \mu \le \sum_{j = 1}^k \frac{j}{k} \mu(B_{j - 1} \cap B_j^c).
\end{equation}

Using that $B_{j-1} \supset B_{j}$ we get
\[
	\mu(B_{j - 1}) = \mu(B_{j-1} \cap B_j^c) + \mu(B_{j-1} \cap B_j) = \mu(B_{j-1} \cap B_j^c) + \mu(B_j)
\]
so that
\[
	\mu(B_{j-1} \cap B_j^c) = \mu(B_{j-1}) - \mu(B_j)
\]

Plugging this into the sum on the right hand side in Equation~\eqref{eq:portmanteau_1} we get
\begin{align*}
	\sum_{j = 1}^k \frac{j}{k} \mu(B_{j - 1} \cap B_j^c) &= \sum_{j = 1}^k \frac{j}{k} (\mu(B_{j-1}) - \mu(B_j))\\
	&= \frac{1}{k} \left(\mu(B_0) + \sum_{j = 1}^{k-1} (j+1) \mu(B_j) - \sum_{j = 1}^k \mu(B_j)\right) \\
	&= \frac{1}{k} \left(1 + \sum_{j = 1}^{k-1} \mu(B_j) - k \mu(b_k)\right)\\
	&= \frac{1}{k} + \frac{1}{k} \sum_{j = 2}^{k} \mu(B_j),
\end{align*}
where we used that $\mu(B_0) = 1$ and $\mu(B_k) = 0$.

In a similar fashion, the sum on the left hand side in Equation~\eqref{eq:portmanteau_1} equals
\[
	\frac{1}{k} \sum_{j = 1}^k \mu(B_j).
\]

We thus conclude that for any $k \ge 1$,
\begin{equation}
	\frac{1}{k} \sum_{j = 1}^k \mu(B_j) \le \int_\bbR h \, \dd \mu \le
	\frac{1}{k} + \frac{1}{k} \sum_{j = 2}^{k} \mu(B_j).
\end{equation}
Moreover, the same inequalities hold for the measures $\mu_n$.

Applying 3 we then get
\begin{align*}
	\limsup_{n \to \infty} \int_\bbR h \, \dd \mu_n
	&\le  \limsup_{n \to \infty} \left(\frac{1}{k} + \frac{1}{k} \sum_{j = 1}^k \mu_n(B_j) \right)\\
	&\le \frac{1}{k} + \frac{1}{k} \sum_{j = 1}^k \limsup_{n \to \infty} \mu_n(B_j)\\
	&\le \frac{1}{k} + \frac{1}{k} \sum_{j = 1}^k \mu(B_j) \\
	&\le \frac{1}{k} + \int_\bbR h \, \dd \mu.
\end{align*}
So that by taking $k \to \infty$ we obtain
\[
	\limsup_{n \to \infty} \int_\bbR h \, \dd \mu_n \le \int_\bbR h \, \dd \mu.
\]

Apply this conclusion to the function $-h$, which is also continuous and bounded, we get
\[
	\int_\bbR h \, \dd \mu \le \liminf_{n \to \infty} \int_\bbR h \, \dd \mu_n,
\]
from which it follows that $\lim_{n \to \infty} \int_\bbR h \, \dd \mu_n = \int_\bbR h \, \dd \mu$.



\textbf{1 $\Rightarrow$ 2:} Fix $\varepsilon > 0$ and let $h^-_\varepsilon$ and $h^+_\varepsilon$ be the function from Lemma [REF]. Then
\[
	\mu \ast h \le \mu \ast h^+_\varepsilon = \mu \ast h^+_\varepsilon - \mu \ast h^-_\varepsilon + \mu \ast h^-_\varepsilon,
\]
which implies that
\[
	 \mu \ast h -\varepsilon \le \mu \ast h^-_\varepsilon.
\]

In a similar way we obtain that
\[
	\mu \ast h^+_\varepsilon \le \mu \ast h +\varepsilon.
\]

Now we employ condition 1 for the functions $h^-_\varepsilon$ and $h^+_\varepsilon$ to get
\begin{align*}
	\mu \ast h -\varepsilon &\le \mu \ast h^-_\varepsilon\\
	&= \lim_{n \to \infty} \mu_n \ast h^-_\varepsilon\\
	&\le \liminf_{n \to \infty} \mu_n \ast h \\
	&\le \limsup_{n \to \infty} \mu_n \ast h\\
	&\le \mu_n \ast h^+_\varepsilon\\
	&= \mu \ast h^+_\varepsilon \le \mu \ast h +\varepsilon.
\end{align*}
From this it follows that
\[
	\mu \ast h -\varepsilon \le \liminf_{n \to \infty} \mu_n \ast h
	\le \limsup_{n \to \infty} \mu_n \ast h \le \mu \ast h +\varepsilon.
\]
And since $\varepsilon > 0$ was arbitrary we conclude that
\[
	\liminf_{n \to \infty} \mu_n \ast h = \limsup_{n \to \infty} \mu_n \ast h,
\]
which then implies that $\mu_n \ast h \to \mu \ast h$.

\textbf{2 $\Rightarrow$ 5:} Let $C$ be a $\mu$-continuity set and consider the function $h(x) = \mathbf{1}_{C}$. Then clearly $h$ is measurable and bounded. Moreover, the function $h$ is discontinuous precisely on the boundary $\partial C$ and hence
\[
	\mu(\cC_h) =\mu(\bbR \setminus \partial C) = 1 - \mu(\partial C) = 1-0 = 1.
\]
Hence the function $h$ satisfies the conditions of 2 and thus
\[
	\mu_n(C) = \int_\bbR h \, \dd \mu_n = \mu_n \ast h \to \mu \ast h = \int_\bbR h \, \dd \mu = \mu(C).
\]


\textbf{5 $\Rightarrow$ 3:} Let $B$ be a closed set, take $\delta > 0$ and consider the sets
\[
	A_\delta = \{x \in \bbR \, : \, \|x - B\| < \delta\},
\]
where $\|x - B\| = \inf_{y \in B} \|x - y\|$ denotes the distance from $x$ to the set $B$. Note that $A_\delta$ is an open set in $\bbR$, and hence $A_\delta^\circ = A_\delta$.

Next we observe that $A_\delta \subset \{x \in \bbR \, : \, \|x-B\| \le \delta\}$ where the latter sets are closed. It then follows that
\[
	\partial A_\delta = \bar{A_\delta} \setminus A_\delta \subset \{x \in \bbR \, : \, \|x-B\| \le \delta\} \setminus A_\delta = \{x \in \bbR \, : \, \|x-B\| = \delta\}.
\]
It then follows that $\partial A_\delta \cap \partial A_{\delta^\prime} = \emptyset$ for all $\delta \ne \delta^\prime$. Since $\mu$ is a probability measure, there can be only a countable number of disjoint sets with positive measure. From this we conclude that there exists a sequence $(\delta_k)_{k \ge 1}$ with $\delta_k \to 0$ such that $\mu(\partial A_{\delta_k}) = 0$ for all $k \ge 1$. Let us write $B_k := A_{\delta_k}$. Then each $B_k$ is a $\mu$-continuity set, $B_k \supset B_{k + 1}$ and $B_k \downarrow B$ because $B$ is closed.

We then have that
\[
	\limsup_{n \to \infty} \mu_n(B) \le \limsup_{n \to \infty} \mu_n(B_k) = \mu(B_k),
\]
where the last equality is due to 5, which implies that $\mu_n(B_k) \to \mu(B_k)$.

Taking $k \to \infty$ now yields 3.


\end{proof}

\subsection{Convergence in distribution}

Convergence in distribution of a sequence $(X_n)_{n }$ is defined as weak convergence of the corresponding probability measures.

\begin{definition}[Convergence in distribution]\label{def:convergence_distribution}
Let $(X_n)_{n \ge 1}$ and $X$ be random variables, possibly defined on different probability spaces with probability measures $\bbP_n$ and $\bbP$, respectively. We say that $X_n$ \emph{converges in distribution} to $X$ if
\[
	(X_n)_\# \bbP_n \Rightarrow X_\# \bbP.
\]
If this is the case write we $X_n \stackrel{d}{\rightarrow} X$.
\end{definition}

Note that convergence in distribution of random variables is simply defined as weak convergence of their push-forward measures on $(\bbR, \cB_\bbR)$. This might seem strange to those who have encountered the \emph{more standard} definition used in courses on Probability Theory. There $X_n \stackrel{d}{\rightarrow} X$ if and only if
\[
	\lim_{n \to \infty} F_n(t) = F(t)
\]
holds for all continuity points $t$ of $F$, with $F_n$ and $F$ denoting the cdfs of $X_n$ and $X$ respectively.

But fear not. It turns out that this definition is yet another equivalent statement for Definition~\ref{def:convergence_distribution}. 


\begin{lemma}\label{lem:convergence_distribution_cdfs}
Let $(X_n)_{n \ge 1}$ and $X$ be random variables and denote by, respectively, $F_n$ and $F$ their associated cdfs. Then
$X_n \stackrel{d}{\rightarrow} X$ if and only if
\[
	\lim_{n \to \infty} F_n(t) = F(t)
\]
holds for all continuity points $t$ of $F$.
\end{lemma}

\begin{proof}
Let $\mu_n := (X_n)_\# \bbP_n$ and $\mu :=  X_\# \bbP_n$ and note that for any measurable function $f$, $\bbE[f(X_n)] = \int f \, \dd \mu_n$ and $\bbE[f(X)] = \int f \, \dd \mu$.

We will first prove that $X_n \stackrel{d}{\rightarrow} X$ implies $\lim_{n \to \infty} F_n(t) = F(t)$ for all $t \in \cC_F$. For this let $h(x) = \mathbf{1}_{(-\infty,t]}$ and note that 
\[
	F_n(t) = (X_n)_\# \bbP_n ((-\infty,x]) = \int_\bbR \mathbbm{1}_{(-\infty,t]} \, \dd (X_n)_\# \bbP_n
	= \mu_n \ast h.
\]
The function $h$ is discontinuous only at $t$, i.e. $\cC_h = \bbR \setminus \{t\}$. Moreover, for any $\varepsilon > 0$
\[
	\mu((t-\varepsilon, t+\varepsilon)) = \mu((t-\varepsilon,t]) + \mu((t,t+\varepsilon))
	= F(t) - F(t-\varepsilon) + F(t + \varepsilon) - F(t).
\]
Since $F$ is continuous at $t$, the right hand side goes to zero as $\varepsilon \to 0$. Therefore
\[
	\mu(\{t\}) = \lim_{\varepsilon \to 0} \mu((t-\varepsilon, t+\varepsilon)) = 0,
\]
which implies that $\mu(\cC_h) = 1$. The result now follows by applying condition 2 in Theorem~\ref{thm:portmanteau}.

Next, we prove the other implication. This will be a bit more advanced as it requires two approximation steps for continuous bounded functions. Suppose that  $\lim_{n \to \infty} F_n(t) = F(t)$ for all $t \in \cC_F$. The goals is to show that
\[
	\lim_{n \to \infty} \|\bbE[f(X_n)] - \bbE[f(X)]\| = 0,
\]
for any continuous bounded function $f$.  

We will first prove this for a slightly different class of functions. Let $h : \bbR \to \bbR$ be a continuous bounded function that is zero outside the closed and bounded interval $[-K, K]]$, for some $K > 0$. We will first show that 
\begin{equation}\label{eq:conergence_distribution_1}
	\lim_{n \to \infty} \|\bbE[h(X_n)] - \bbE[h(X)]\| = 0.
\end{equation}

First note that since $h$ is bounded and continuous and $[-K,K]$ is closed There exists a $\delta > 0$ such that $\|x-y\| \le \delta$ implies that $\|h(x) - h(y)\| < \varepsilon$.

So fix $\varepsilon > 0$, pick such a $\delta$ and partition the interval $[-K,K]$ into $L_\delta := \left \lceil \frac{4K}{\delta} \right \rceil$ intervals $I_\ell = (a_\ell, b_\ell]$ of equal length, which is $\le \delta/2 < \delta$. Now we define the simple function
\[
	\hat{h} := \sum_{\ell = 1}^L h(b_\ell) \mathbbm{1}_{I_\ell},
\]
and note that we can write this as
\[
	\hat{h} := \sum_{\ell = 1}^L \beta_\ell \mathbf{1}_{(-\infty, b_\ell]},
\]
where $\beta_\ell = \sum_{t = 1}^\ell h(b_t)$. 

With this last representation we get
\begin{align*}
	\lim_{n \to \infty} \bbE[\hat{h}(X_n)] 
	&= \lim_{n \to \infty} \sum_{\ell = 1}^L \beta_\ell \int_\bbR \mathbf{1}_{X_n^{-1}((-\infty,b_\ell])} \, \dd \bbP \\
	&= \lim_{n \to \infty} 	\sum_{\ell = 1}^L \beta_\ell (X_n)_\# \bbP ((-\infty, b_\ell]) \\
	&= \lim_{n \to \infty} 	\sum_{\ell = 1}^L \beta_\ell F_n(b_\ell) \\
	&= \sum_{\ell = 1}^L F(b_\ell) = \bbE[\hat{h}(X)].
\end{align*}

Using the first representation of $\hat{h}$ we note that $\|h(x) - \hat(y)\| < \varepsilon$ for all $x,y \in I_\ell$. This then implies that
\begin{align*}
	\|\bbE[h(X_n)] - \bbE[h(X)]\| &\le \|h(X_n) - \hat{h}(X_n)\| + \|h(X) - \hat{h}(X)\| + \|\hat{h}(X_n) - \hat{h}(X)\|\\
	&\le 2\varepsilon + \|\hat{h}(X_n) - \hat{h}(X)\|,
\end{align*}
where we have shown that the last term goes to zero as $n \to \infty$. Since $\varepsilon$ was arbitrary we conclude that~\eqref{eq:conergence_distribution_1} holds.

Now consider the case of a continuous bounded function $f$ with $\|f(x)\| \le M$ for all $x \in \bbR$. We will show that for any $\varepsilon > 0$
\[
	\|\bbE[f(X_n)] - \bbE[f(X)]\| \to \varepsilon,
\]
which then implies the result.

So let $\varepsilon > 0$ be fixed and observe that there exists an $\alpha > 0$ such that $\bbP(\|X|\ > \alpha) \le \varepsilon/(2M)$. Also observe that we can define a non-negative continuous function $g$ such that $g = 1$ on $[-\alpha, \alpha]$ and $g = 0$ on $\bbR \setminus (-(\alpha+1),\alpha+1)$.

We now make two observation
\begin{enumerate}
\item The function $g$ is a non-negative continuous bounded functions that is zero outside the interval $[-(\alpha+1), \alpha+1]$ and thus we can apply the convergence result in~\eqref{eq:conergence_distribution_1}.
\item By definition of $\alpha$ we have that 
\[
	\bbE[g(X)] \ge \bbE[g(X) \mathbbm{1}_{X \in [-\alpha, \alpha]}] = \bbP(X^{-1}([-\alpha,\alpha]))
	= 1 - \bbP(\|X\| > \alpha) \ge 1 - \frac{\varepsilon}{2M}.
\] 
\end{enumerate}

Let us now consider the function $f(X_n)$ and $f(X_n)g(X_n)$. Then we have that
\begin{align*}
	\|\bbE[f(X_n)] - \bbE[f(X_n)g(X_n)]\| \le \bbE[f(X_n)\|1 - g(X_n)\|] 
	\le M \bbE[1 - g(X_n)] = M(1-\bbE[g(X_n)]).
\end{align*}
Since the later term converges to $\bbE[g(X)]$ by~\eqref{eq:conergence_distribution_1} we get that
\[
	\limsup_{n \to \infty} \|\bbE[f(X_n)] - \bbE[f(X_n)g(X_n)]\| \le M(1-\bbE[g(X)])
	\le \frac{\varepsilon}{2},
\]
using the second observation. The same conclusion holds true when we replace $X_n$ by $X$.

If we now write
\begin{align*}
	\|\bbE[f(X_n)] - \bbE[f(X)]\| &\le \|\bbE[f(X_n)] - \bbE[f(X_n)g(X_n)]\| + \|\bbE[f(X)g(X)] - \bbE[f(X)]\| \\
		&\hspace{10pt}+ \|\bbE[f(X_n)g(X_n)] - \bbE[f(X)g(X)]\|
\end{align*}
we see that the first two terms converge to $\varepsilon/2$ (by the computation above) while the term on the second line converges to zero by~\eqref{eq:conergence_distribution_1} since $fg$ is also a continuous bounded function that is zero outside the interval $[-(\alpha+1),\alpha+1]$. 
\end{proof}



\subsection{Convergence in probability}

\begin{definition}[Convergence in probability]
Let $(X_n)_{n \ge 1}$ and $X$ be random variables that are defined on the \emph{same} probability space $(\Omega, \cF, \bbP)$ and define the random variable $Y_n := \|X_n - X\|$. We say that $X_n$ \emph{converges in probability} to $X$ if
\[
	(Y_n)_\# \bbP \Rightarrow 0_\# \bbP,
\] 
where $0$ denotes the constant function $\omega \mapsto 0$.

If this is the case we write $X_n \stackrel{\bbP}{\rightarrow} X$.
\end{definition}

Recall that for a random variable $X$ on a probability space $(\Omega, \cF, \bbP)$ we wrote $\bbP(X \le t)$ as a short hand notation for $X_\# \bbP ((-\infty,t])$, i.e. the cdf of $X$ at $t$, and $\bbP(X > t)$ for $X_\# \bbP ((t,\infty))$, i.e. the ccdf of $X$ at $t$.

The following result relates the definition of convergence in probability to a version that is presented in most probability courses.

\begin{lemma}\label{lem:convergence_probability_classical}
Let $(X_n)_{n \ge 1}$ and $X$ be random variables define on the same probability space. Then $X_n \stackrel{\bbP}{\rightarrow} X$ if and only if
\[
	\lim_{n \to \infty} \bbP(\|X_n - X\| > \varepsilon) = 0 \quad \text{for every } \varepsilon > 0.
\]
\end{lemma}

\begin{proof}

\end{proof}


The notion of convergence in probability is a stronger condition than convergence in distribution. In particular, the first statement implies the second. 

\begin{lemma}
Let $(X_n)_{n \ge 1}$ and $X$ be random variables such that $X_n \plim X$. Then $X_n \dlim X$.
\end{lemma}

\begin{proof}
Denote by $F_n$ and $F$ the cdfs of $X_n$ and $X$, respectively. We will also write $\bbP(X > t)$ Let $t$ be a continuity point of $F_X$ and fix some $\varepsilon > 0$. First we note that if $X_n \le t$ and $|X-X_n|\le \varepsilon$ then $X \le t + \varepsilon$. We thus obtain
\begin{align*}
	\bbP(X_n \le t)
	&= \bbP(\{X_n \le t\} \cap \{|X_n - X| \le \varepsilon\}) + \bbP(\{X_n \le t\} \cap \{|X_n - X|>\varepsilon\})\\
	&\le \bbP(X \le t + \varepsilon) + \bbP(|X_n - X| > \varepsilon).
\end{align*}
Taking the limsup on both sides yields
\[
	\limsup_{n \to \infty} \bbP(X_n \le t)
	\le \bbP(X \le t + \varepsilon) + \limsup_{n \to \infty} \bbP(\|X_n - X| > \varepsilon)
	= \bbP(X \le t + \varepsilon),
\]
since $X_n \plim X$ implies that
\[
	\limsup_{n \to \infty} \bbP(|X_n - X| > \varepsilon)
	= \lim_{n \to \infty} \bbP(|X_n - X| > \varepsilon) = 0.
\]
Since $t$ is a continuity point of $F_X$ it follows that 
\[
	\lim_{\varepsilon \downarrow 0} \bbP(X \le t + \varepsilon) = \bbP(X \le t),
\]
which implies that $\limsup_{n \to \infty} \bbP(X_n \le t) \le \bbP(X \le t)$.

To prove the result, it now suffices to show that $\bbP(X \le t) \le \liminf_{n \to \infty} \bbP(X_n \le t)$.
We shall do this in a way that is similar to the case with the limsup. First observe that $X \le t - \varepsilon$ and $|X_n - X| \le \varepsilon$ implies that $X_n \le t$. This way we get
\begin{align*}
	\bbP(X \le t - \varepsilon)
	&= \bbP(\{X \le t - \varepsilon\} \cap \{|X_n -X|\le \varepsilon\}) + \bbP(\{X \le t - \varepsilon\} \cap \{|X_n - X|>\varepsilon\})\\
	&\le \bbP(X_n \le t) + \bbP(|X_n - X| > \varepsilon).
\end{align*}
Taking the liminf on both sides gives
\[
	\bbP(X \le t - \varepsilon)
	\le \liminf_{n \to \infty} \bbP(X_n \le t),
\]
and using that $t$ is a continuity point of $F_X$ we conclude that 
\[
	\bbP(X \le t) \le \liminf_{n \to \infty} \bbP(X_n \le t).
\]
\end{proof}

While convergence in probability implies convergence in distribution, the other implication is not true in general (see Problem ??). However, if $X$ is constant (deterministic instead of random) then both notions of convergence are equivalent.

\begin{lemma}{}\label{lem:dlim_constant_plim}
Let $(X_n)_{n \ge 1}$ be a sequence of random variables such that $X_n \dlim a$ for some constant $a \in \mathbb{R}$. Then we also have that $X_n \plim a$.
\end{lemma}

\begin{proof}
Fix some $\varepsilon > 0$. We have to show that
\[
	\lim_{n \to \infty} \bbP(|X_n-a|>\varepsilon) = 0.
\]
Let $B_\varepsilon(a)$ denote the open ball of radius $\varepsilon$ around $a$ and consider the compliment $B_\varepsilon(a)^c := \mathbb{R} \setminus B_\varepsilon(a)$, which is a closed set. We then have
\[
	\bbP(|X_n-a|>\varepsilon) \le \bbP(|X_n-a|\ge\varepsilon) = \bbP(X_n \in B_\varepsilon(a)^c). 
\]
In particular we have
\[
	\lim_{n \to \infty} \bbP(|X_n-a|>\varepsilon) \le \limsup_{n \to \infty} \bbP(|X_n-a|>\varepsilon)
	\le \limsup_{n \to \infty} \bbP(X_n \in B_\varepsilon(a)^c).
\]
Since $X_n \dlim a$, statement 3 in Theorem~\ref{thm:portmanteau} implies that 
\[
	\limsup_{n \to \infty} \bbP(X_n \in B_\varepsilon(a)^c)
	\le \limsup_{n \to \infty} \bbP(a \in B_\varepsilon(a)^c) = 0,
\]
because obviously $a \notin B_\varepsilon(a)^c$. Therefore we conclude that
\[
	\lim_{n \to \infty} \bbP(|X_n-a|>\varepsilon)
	\le \limsup_{n \to \infty} \bbP(a \in B_\varepsilon(a)^c) = 0
\]
which implies that $\lim_{n \to \infty} \bbP(|X_n-a|>\varepsilon) = 0$.
\end{proof}


\subsection{Almost-sure convergence}

\section{Problems}

\begin{problem}\label{prb:convergence_distribution}
The goal of this problem is to prove Lemma~\ref{lem:convergence_probability_classical}. That is
\[
	X_n \stackrel{d}{\rightarrow} X \iff F_n(t) \to F(t) \quad \text{for all } t \in \cC_F,
\]
where $F_n$ and $F$ denote the cdfs of the random variables $X_n$ and $X$, respectively.
 
We will first prove the $\Rightarrow$ implication. Write $\mu_n = (X_n)_\# \bbP_n$ and $\mu = X_\# \bbP$.
\begin{enumerate}
\item Let $t \in \bbR$. Find a measurable function $h_t$, such that $F_n(t) = \mu_n \ast h_t$ and $F(t) = \mu \ast h_t$.
\item Show that $\mu(\cC_{h_t}) = 1$.
\item Prove the $\Rightarrow$ implication. 
\end{enumerate}

For the other implication $\Leftarrow$ we will first show that $F_n(t) \to F(t)$ for all $t \in \cC_F$ implies that $\mu_n \ast g \to \mu \ast g$ for all continuous function that are non-zero only on a bounded and closed set $C$.

So let $g$ be such a function. You may use the fact that any continuous function that is non-zero on a bounded and closed set is uniformly continuous, i.e. for every $\varepsilon > 0$ there exist a $\delta > 0$ such that $\|x-y\| < \delta$ implies that $\|g(x) - g(y)\| < \varepsilon$. 
\begin{enumerate}
\setcounter{enumi}{3}
\item Construct a partition of $C$ into $T$ intervals $I_i = (a_i, b_i]$ such that for each $i \le T$ and $x, y \in I_i$ it holds that $\|x - y\| < \delta$.
\end{enumerate}

We will now define an approximate function
\[
	\hat{g}(x) = \sum_{i = 1}^T \eta_i \mathbf{1}_{I_i},
\]
where $\eta_i = h(\max\{x \in I_i\})$.

\begin{enumerate}
\setcounter{enumi}{4}
\item Show that there exists sequences $(b_i)_{1 \le i \le m}$ and $(t_i)_{1 \le i \le m}$ such that
\[
	\tilde{g}(x) = \sum_{i = 1}^m b_i \mathbf{1}_{(-\infty, t_i]}.
\]
\item Prove that $\lim_{n \to \infty} \bbE[\tilde{g}(X_n)] = \bbE[\tilde{g}(X)]$. [Hint: Use the assumption $F_n(t) \to F(t)$ for all $t \in \cC_F$.]
\item Prove that $\|\bbE[g(X_n)] - \bbE[g(X)]\| \to 0$. [Hint: First use the previous result to show that $\|\bbE[g(X_n)] - \bbE[g(X)]\| \to 2\varepsilon$ by adding and subtracting $\bbE[\tilde{g}(X_n)]$ and $\bbE[\tilde{g}(X)]$.]
\end{enumerate}

Now consider a general continuous bounded function $h$ and suppose that $\|h(x)\| \le H$ for all $x \in \bbR$. Let $\varepsilon > 0$, let $\alpha = \alpha(\varepsilon) > 0$ be such that $\bbP(\|X\| > \alpha) \le \varepsilon/(2H)$ and define
\[
	\hat{h}(x) = \begin{cases}
		x + 1 + \alpha g&\text{if } -\alpha - 1 < x < -\alpha,\\
		1 &\text{if } \|x\| \le \alpha,\\
		x + (1-\alpha) &\text{if } \alpha < \alpha < \alpha +1,\\
		0 &\text{else.}
	\end{cases}
\]
\begin{enumerate}
\setcounter{enumi}{7}
\item Show that $\bbE[\hat{h}(X)] \ge 1 - \frac{\varepsilon}{2M}$.
\item Prove that $\left|\bbE[X_n] - \bbE[h(X_n)\hat{h}(X_n)] \right| \to \varepsilon/2$ and similarly for $X$.
\item Show that $\left|\bbE[h(X_n)] - \bbE[h(X)]\right| \to \varepsilon$. [Hint: add and subtract both $\bbE[h(X_n)\hat{h}(X_n)]$ and $\bbE[h(X)\hat{h}(X)]$ and use the previous result and the one show in 7.]
\item Conclude that $\bbE[h(X)_n)] \to \bbE[h(X)]$.
\end{enumerate} 

\end{problem}

