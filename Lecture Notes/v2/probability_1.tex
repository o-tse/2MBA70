
\section{Random variables and general stochastic objects}\label{sec:random_variables}

% Random variables   

\subsection{Definition}

In the course Probability and Modeling two types of random variables were defined: discrete and continuous. Recall that a random variable was defined as a function $X : \Omega \to \bbR$ for some probability space $(\Omega, \cF, \bbP)$ such that
\[
	\{\omega \in \Omega \, : \, X(\omega) \le x\} \in \cF \quad \text{for all } x \in \bbR.
\]
Let us make two observations here. The first is that the set above is simply the preimage $X^{-1}((-\infty,x])$. Secondly, the sets $(-\infty, x]$ generate the Borel \sigalg/. Thus it follows from Lemma~\ref{lem:measurable_condition_generator} that $X$ is a measurable function. This is actual the proper way to define a random variable.

\begin{definition}[Random variable]
A \emph{random variable} is a measurable function from some probability space $(\Omega,\cF, \bbP)$ to the Borel space $(\bbR, \cB_{\bbR})$.
\end{definition}

It is important to observe that the definition of a random variable does not make any specific claims on what the probability space should be. 

Let $X$ be a random variable and recall that its \emph{cumulative distribution function} $F_X : \bbR \to [0,1]$ is defined as
\[
	F_X(t) = \bbP(X \le t).
\] 
The fact that we use $\bbP$ here, which is the probability measure on the space $(\Omega, \cF)$ is not a coincidence.

The idea behind the cdf $F_X(t)$ is that it denotes the "probability" that $X \in (-\infty ,t]$. From the perspective of measure theory, this means we need to assign a measure to the preimage of $(-\infty, t]$ under the measurable function $X$. For this, the only things we have at our disposal is the probability measure $\bbP$ and the measurable function $X$. Now recall from Proposition~\ref{prop:push_forward_measure} that we can always construct a measure from this, the push-forward measure. That is exactly what the cummulative distribution is,
\[
	F_X(t) := X_\# \bbP((-\infty, t]) = \bbP(X^{-1}((-\infty,t])).
\]

In fact we can actually define, at a much more general level, random elements in any measurable space and put an associated probability measure on this space by a push-forward.

\begin{definition}[Random elements]
Let $(\Omega,\cF, \bbP)$ be a probability space and $(E,\cG)$ some measurable space. A \emph{random element} in $(E,\cG)$ is a measurable map $X : \Omega \to E$. It associated \emph{probability measure} is defined as the push forward $X_\# \bbP$ of $\bbP$ under $X$, i.e.
\[
	\bbP(X \in A) := \bbP(X^{-1}(A)) \quad \text{for every } A \in \cG.
\]
\end{definition}

Sometimes we use the term \emph{stochastic} instead of \emph{random}. 

With this general definition we can now easily define random vectors, random matrices, random functions and so one. The only thing we need is to start with the appropriate space (vectors, matrices, functions) and turn it into a measurable space by endowing it with a suitable \sigalg/. 

\begin{example}[Random elements]
\hfill
\begin{enumerate}[label=(\alph*)]
\item A random vector in $\bbR^d$ is a random element in $(\bbR^d, \cB_{\bbR^d})$.
\item A random $n \times m$ matrix is a random element in $(\bbR^n \times \bbR^m, \cB_{\bbR^n} \otimes \cB_{\bbR^m})$.
\end{enumerate}
\end{example}

While these are somewhat straightforward examples, there are also more involved ones that are important in probability theory.

\begin{example}[Stochastic processes]
Let $(\Omega,\cF,\bbP)$ be a probability space, $(S,\cS)$ a measurable space and $T$ some index set. Then we denote by $S^T$ the set of all functions $f : T \to S$. For any $t \in T$, denote by $\pi_t : S^T \to S$ the \emph{evaluation function} $\pi_t(f) = f(t)$. Then we endow the space $S^T$ with the \sigalg/ $\cS^T := \sigma(\pi_t \, : \, t\in T)$. A \emph{stochastic process} on $(S,\cS)$ is then defined as a measurable function $X : (\Omega,\cF) \to (S^T, \cS^T)$.

The space $(S,\cS)$ is often called the \emph{state space} of the stochastic process. Often, the index set is taken to be $\bbN$ or $\bbR_{\ge 0}$. However, the construction above allows for more exotic index sets (although this might impact the properties of the associated stochastic processes).
\end{example}

\subsection{Constructing random variables}

Now that we know what random variables are, there is one problem. In order to define an random variable we need to formally define a probability space $(\Omega, \cF, \bbP)$ and measurable function $X: \Omega \to \bbR$. This is different from how we are used to work with random variables. Here we simply present a cdf $F$ and say that $X$ is a random variable with $\bbP(X \le t) = F(t)$, without worrying about a probability space or the measurability if $X$ as a function. It turns out that this way of working with random variables is valid, as for any cdf $F$ we can construct a probability space $(\Omega, \cF, \bbP)$ and measurable function $X$ such that $X_\# \bbP = F$. We will start this construction for a specific random variable and then use it to construct a random variable with any cumulative distribution function.

One of the first random variables you encounter in any course in probability theory is the \emph{standard uniform random variable}. This is a random variable $U$ that takes values in $[0,1]$ such that its cdf satisfies $F(t) = t$ for all $0\le t \le 1$. In the course Probability and Modeling this description would be enough to work with. But now that we know what a random variable actually is, we need a bit more. More precisely, we have to construct a probability space $(\Omega,\cF, \bbP)$ and a measurable function $U : \Omega \to \bbR$ such that 
\begin{equation}\label{eq:cdf_uniform_rv}
	\bbP\left(U^{-1}((-\infty,t])\right) = \begin{cases}
		0 &\text{if } t < 0,\\
		t &\text{if } 0 \le t\le 1,\\
		1 &\text{if } t > 1.
	\end{cases}
\end{equation}

The following result shows that this is indeed possible. Moreover, in its proof we see a first nice usage of the Lebesgue measure.

\begin{proposition}[Uniform random variable]\label{prop:uniform_random_variable}
There exist a probability space $(\Omega,\cF, \bbP)$ and random variable $U$, such that $\bbP\left(U^{-1}((-\infty,t])\right)$ satisfies~\eqref{eq:cdf_uniform_rv}.
\end{proposition}

\begin{proof}
Consider the space $\Omega = [0,1]$ together with the restricted Borel \sigalg/ $\cF = \cB_{[0,1]}$ and as probability measure the restricted Lebesgue measure $\bbP := \lambda|_{[0,1]}$. Now consider the function $U(t) = \mathbf{1}_{(0,1]} \, t$. Then, it follows that
\[
	U^{-1}((-\infty,t]) = \begin{cases}
		\emptyset &\text{if } t \le 0,\\
		(0,t] &\text{if } 0 < t \le 1, \\
		[0,1] &\text{if } t > 1.
	\end{cases}
\]
Since by Theorem~\ref{thm:lebesgue_measure}
\[
	\lambda|_{[0,1]}((0,t]) = \lambda((0,t]) = t,
\]
for any $0 < t \le 1$ we have
\[
	\bbP\left(U^{-1}((-\infty,t])\right) := \lambda|_{[0,1]}\left(U^{-1}((-\infty,t])\right)
	= \begin{cases}
		0 &\text{if } t < 0,\\
		t &\text{if } 0 \le t\le 1,\\
		1 &\text{if } t > 1.
	\end{cases}\qedhere
\]
\end{proof}

The standard uniform random variable is extremely important, as it is the base from which we can construct any other random variable. To illustrate this let us first consider the case of an \emph{exponential random variable} with rate $\lambda > 0$. This is a random variable $X$ with cdf
\[
	F_X(t) = \begin{cases}
		0 &\text{if } t \le 0,\\
		1-e^{-\lambda t} &\text{if } t > 0.
	\end{cases}
\]

For $u \in (0,1)$, write $H(u) := F_X^{-1}(u)$ and note that
\[
	H(u) = \frac{1}{\lambda} \log\left(\frac{1}{1-u}\right).
\]
Now let $U$ be a standard uniform normal random variable and consider the composition $H \circ U : [0,1] \to \bbR$. First, we note that since cdf $F_X(x)$ is strictly monotonic increase, so is $H$. In particular, it follows that for any $t > 0$,
\[
	H^{-1}((-\infty,t]) = (-\infty, H^{-1}(t)] = (-\infty, F_X(t)].
\]
While $H^{-1}((-\infty,t]) = \emptyset$ if $t \le 0$.

Hence we get
\begin{align*}
	(H \circ U)^{-1}((-\infty, t]) = U^{-1}(H^{-1}((-\infty, t]))
	&= \begin{cases}
		U^{-1}(\emptyset) &\text{if } t \le 0,\\
		U^{-1}((-\infty, F_X(t)]) &\text{if } t > 0.
	\end{cases}
\end{align*}
From this it follows that 
\[
	\bbP\left((H \circ U)^{-1}((-\infty, t])\right) = \begin{cases}
			0 &\text{if } t \le 0,\\
			1-e^{-\lambda t} &\text{if } t > 0,
		\end{cases}
\]
from which we conclude that $H \circ U$ is a way to construct an exponential random variable with rate $\lambda$.

The main point of the construction above is to consider the inverse of the cdf $F^{-1}$ and evaluate this on a standard uniform random variable. However, when extending this to the more general case we have to deal with the fact that not every cdf has an inverse. For example, consider a Bernoulli random variable with success probability $0 < p < 1$. Then
\[
	F(t) = \begin{cases}
		0 &\text{if } t < 0, \\
		1-p &\text{if } 0 \le t < 1, \\
		1 &\text{if } t \ge 1,
	\end{cases}
\] 
which does not have an inverse as for any $y \in (0,1-p)$ there is no $t$ such that $F(t) = y$.

Nevertheless, it does hold that any cdf $F$ is non-decreasing and right continuous. For these types of functions, there exists the notion of a \emph{generalized inverse}, defined as
\begin{equation}
	\overleftarrow{F}(u) := \inf\{ x \in \bbR \, : \, F(x) \ge y\}. 
\end{equation}
The construction we used for the exponential random variable can now be generalized by using $\overleftarrow{F}$ instead of $F^{-1}$. This results in the following theorem on the existence of random variables with a given cdf.

\begin{theorem}[Constructing random variables]\label{thm:construction_random_variable}
Let $F : \bbR \to [0,1]$ be a right continuous, non-decreasing function with 
\[
	\lim_{x \to -\infty} F(x) = 0 \quad \text{and} \quad \lim_{x \to \infty} F(x) = 1.
\]
Then there exists a probability space  $(\Omega,\cF, \bbP)$ and random variable $X$, such that 
\[
	\bbP\left(X \in (-\infty,t]\right) := \bbP\left(X^{-1}((-\infty,t])\right) = F(t).
\]
In other words, $X$ is a random variable with cdf $F$.

Moreover, $(\Omega,\cF, \bbP)$ can be chosen as $([0,1], \cB_{[0,1]}, \lambda|_{[0,1]})$ and $X = \overleftarrow{F}\circ U$, where $U$ is the standard uniform random variable.
\end{theorem}

\begin{proof}
We start with the following important observation: 
\[
	\overleftarrow{F}(u) \le x \iff F(x) \ge u.
\]
The implication from right to left is by definition of $\overleftarrow{F}$ and the fact that $F$ is non-decreasing. The implication from left to right is because $F$ is right continuous.


Now let $(\Omega,\cF, \bbP)$ be a probability space and $U$ a standard normal random variable. We will show that $X = \overleftarrow{F} \circ U$ is a random variable with the right probability measure. Since we can construct a standard uniform random variable on the probability $([0,1], \cB_{[0,1]}, \lambda|_{[0,1]})$ this also implies the last part. 

Consider the preimage of $(-\infty, t]$ under $X$. Then, using the above observation, we have
\begin{align*}
	X^{-1}((-\infty,t]) &= \{\omega \in \Omega \, : \, \overleftarrow{F}(U(\omega)) \in (-\infty,t]\}\\
	&= \{\omega \in \Omega \, :\ , U(\omega) \in (-\infty, F(t)]\} = U^{-1}((-\infty, F(t)]) \in \cB_{[0,1]}.
\end{align*}
Hence, $X$ is measurable. Finally, the above computation, together with Proposition~\ref{prop:uniform_random_variable}, also implies that
\[
	\bbP\left(X^{-1}((-\infty,t])\right) = \bbP\left(U^{-1}((-\infty, F(t)])\right) = F(t),
\]
which finished the proof.
\end{proof}

We end this section with an important remark for working with random variables, and random objects in general.

\begin{remark}[Probability spaces are implicit!]
It is important to note that even though we used a very explicit probability space to construct a standard uniform random variable and the random variable $X$ in the proof of Theorem~\ref{thm:construction_random_variable}, in general the probability space will often be \emph{implicit}. That is, if we say that $X$ is a random variable, we assume there is some probability space $(\Omega,\cF, \bbP)$ that makes $X$ into a measurable function with the right cdf. Theorem~\ref{thm:construction_random_variable} actually says that this is okay as we can always construct an appropriate probability space and measurable function to achieve the needed cdf.

Actually, when considering general random objects in $(E, \cG)$ we often also do not explicitly state or define the probability space. Since the relevant measure is defined through the push-forward we often only have to worry about taking the right measurable space $(E, \cG)$.

There are, however, some cases where one should be cautious about the probability space that is used. For example when considering the notion of \emph{convergence in probability} or \emph{almost sure convergence}. Or when constructing joint distributions of random variables. 
\end{remark}

\section{Multi-variate random variables}

[TODO]

\section{Expectation of random variables}

[REWRITE]

Now that we have a notion of integration we can formally define what the expected value of a random variables is.

\begin{definition}\label{def:expectation_random_variable}
Let $(\Omega. \cF, \bbP)$ be a probability space and $X$ and random variable. Then
\[
	\bbE[X] := \int_\Omega X \, \dd \bbP.
\]
\end{definition}

We say that a random variable is discrete if $X(\omega) \in \bbZ$ for all $\omega \in \Omega$. It then follows (see Problem~\ref{prb:discrete_random_variable}) that 
\[
	\bbP(X \in A) = \sum_{j \in \bbZ} \delta_j(A) p_j,
\]
for some sequence $(p_j)_{j \in \bbZ}$ with $\sum_{j \in \bbZ} p_j = 1$. We can now define the \emph{probability mass function} (pmf) of $X$ as $p(j) = p_j$.

In the course Probability and Modeling you have seen the following definition of the expectation of $h(X)$, where $h$ is a function and $X$ a discrete random variable:
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} j \bbP(h(X) = j).
\]
The following result, called the law of the unconscious statistician, expressed the expectation in terms of the probabilities pmf of $X$:
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} h(j) p(j).
\] 

We will now use the change of variables proposition to prove this result, given the general definition for expectations in Definition~\ref{def:expectation_random_variable}.


\begin{lemma}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space, $X$ be a discrete random variable and consider a function $h : \bbR \to \bbR$ such that $h\circ X$ is $\bbP$-integrable. Then
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} h(j) p(j),
\] 
where $p$ is the pmf of $X$.
\end{lemma}

\begin{proof}
Recall the definition of the positive and negative part of a measurable function $f$, denoted by respectively $f^+$ and $f^-$. Further, recall that 
\[
	\int_\Omega f \, \dd \mu := \int_\Omega f^+ \, \dd \mu - \int_\Omega f^- \, \dd \mu 
\] 
Now, for any $n \in \bbN$ define the functions
\[
	g_n^\pm = \sum_{j = -n}^n (h \circ X)^\pm \mathbf{1}_{X^{-1}(j)}.
\]
Then $g_n^\pm \le g_{n+1}^\pm$ and
\[
	\lim_{n \to \infty} g_n^\pm = (h \circ X)^\pm.
\]
Then, using the monotone convergence theorem we get
\begin{align*}
	\int_\Omega (h \circ X)^\pm \, \dd \bbP &= \int_\Omega \lim_{n \to \infty} g_n^\pm \, \dd \bbP\\
	&= \lim_{n \to \infty} \int_\Omega g_n^\pm \, \dd \bbP \\
	&= \lim_{n \to \infty} \sum_{j = -n}^n \int_\Omega (h \circ X)^\pm \mathbf{1}_{X^{-1}(j)} \, \dd \bbP\\
	&= \lim_{n \to \infty} \sum_{j = -n}^n \int_{X^{-1}(j)} h^\pm(j) \, \dd \bbP\\
	&= \lim_{n \to \infty} \sum_{j = -n}^n h^\pm(j) \bbP(X^{-1}(j)) \\
	&= \lim_{n \to \infty} \sum_{j = -n}^n h^\pm(j) p(j) = \sum_{j \in \bbZ} h^\pm(j) p(j).
\end{align*}
Since $h\circ X$ is $\bbP$-integrable if and only if it positive and negative part are, we conclude that
\[
	\int_\Omega (h \circ X) \, \dd \bbP = \int_\Omega (h \circ X)^+ \, \dd \bbP
	- \int_\Omega (h \circ X)^- \, \dd \bbP = \sum_{j \in \bbZ} h(j) p(j).
\]
\end{proof}

Let us now turn to the other class of random variables, continuous random variables. Here the notion of a \emph{probability density function} $\rho$ was introduce so that $F(t)$ was equal to the integral of $\rho$ on $(-\infty,t]$. Expressed formally in the language of measure theory we have the following

\begin{definition}[Probability density function]
Let $(\Omega, \cF, \mathbb{P})$ be a probability space and $X$ a continuous random variable. We say that $X$ has a \emph{probability density function} $\rho : \bbR \to [0,\infty)$, if for every $t \in \bbR$,
\[
	X_\# \bbP((-\infty , t]) = \int_{(-\infty,t]} \rho \, \dd \lambda.
\]
In particular, a probability density function must be integrable.
\end{definition}

Now recall that in the case of a continuous random variable $Y$ with a probability density $\rho$, there was also a formula for its expectation,
\[
	\bbE[h(Y)] = \int_\bbR h(x) \rho(x) \, \dd x.
\]
Again, this formula is correct and follows from Definition~\ref{def:expectation_random_variable} after applying a change of variables. The proof of this result is left as an exercise.

\begin{lemma}\label{lem:expectation_continuous_random_variable}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space, $X$ a continuous random variable with probability density $\rho$ and let $h : \bbR \to \bbR$ be a measurable function such that $h \rho$ is Lebesgue integrable. Then
\[
	\bbE[h(Y)] = \int_\bbR h \rho \, \dd \lambda.
\]
\end{lemma}

\begin{proof}
See problem~\ref{prb:expectation_continuous_random_variable}
\end{proof}

\section{The Markov inequality}

The following result states the Markov inequality. The trick used in the proof can be used to obtain many similar inequalities.

\begin{lemma}[The Markov inequality]
	Let $(\Omega, \cF, \mu)$ be a measurable space and let $f$ be a $\mu$-integrable function. For any $t>0$,
	\[
	\mu\bigl(\{ \omega \in \Omega : \ |f(\omega)| \geq t \}\bigr) \leq \frac{1}{t} \int_\Omega |f| \,\dd \mu.
	\]
\end{lemma}
\begin{proof} The result follows easily from
	\[
		\int_\Omega |f|\,\dd \mu \ge \int_{\{|f|\ge t\}} |f|\,\dd \mu \ge t\,\mu\bigl(\{|f|\ge t\}\bigr)  \qedhere
	\]
\end{proof}


In probability language, the Markov inequality looks as follows.

\begin{lemma}[The Markov inequality]
	Let $(\Omega, \cF, \mathbb{P})$ be a probability space and let $X$ be a random variable. For any $t>0$,
	\[
	\mathbb{P}(|X| \geq t) \leq \frac{1}{t} \mathbb{E}[|X|].
	\]
\end{lemma}

\section{Problems}

\begin{problem}
Let $X,Y$ be two random variables with cdfs $F_X$ and $F_Y$, respectively. 
\begin{enumerate}[label=(\alph*)]
\item Prove that if $F_X(t) = F_Y(t)$ for every $t \in \bbR$, then $X_\# \bbP = Y_\# \bbP$. 
\end{enumerate}
The above relation is often denoted as $X \stackrel{d}{=} Y$ ($X$ is equal to $Y$ in distribution). Basically, this definition says that two random variables are considered equal if their distribution functions are the same, which is implied by the equality of their cdfs. However, the use of the equality sign can be slightly misleading.
\begin{enumerate}
\setcounter{enumi}{1}
\item Let $X$ be a random variable. Construct a random variable $Y$ such that $X_\# \bbP = Y_\# \bbP$, but $X \ne Y$ as functions $\Omega \to \bbR$.
\end{enumerate}
\end{problem}

\begin{problem}
In this problem we will	explicitly construct two random variables $X$ and $Y$ such that $X$ is \emph{Poisson} distributed with parameter $\lambda>0$, and $Y$ is \emph{Cauchy} distributed with parameter $\gamma>0$. 

For this we define the Poisson probability mass function (pmf)
\begin{equation}
	f_\lambda(n) := \frac{e^{-\lambda}\lambda^n}{n!},\qquad \forall\,n\in\bbN,
\end{equation}
and the Cauchy cumulative distribution function (cdf)
\begin{equation}
	H_\gamma(z) := \frac{1}{\pi}\arctan\left(\frac{z}{\gamma}\right) + \frac{1}{2},\qquad \forall\,z\in\bbR.
\end{equation}

We will first construct the Cauchy random variable.
\begin{enumerate}
\item Define an explicit probability space $(\Omega, \cF, \bbP)$ and provide an explicit formula for the function $Y\colon \Omega\to\bbR$ such that $Y_\# \bbP = H_\gamma$.
\item Show that the function from 1 is measurable.
\end{enumerate}

For the Poisson random variable we first need to go from the pmf to its cdf.
\begin{enumerate}
\setcounter{enumi}{2}
\item Express the cumulative distribution function $F_\lambda$ for a Poisson random variable in terms of the pmf $f_\lambda$.
\item Define an explicit probability space $(\Omega, \cF, \bbP)$ and provide a formula for the function $X\colon \Omega\to\bbR$ such that $X_\# \bbP = F_\lambda$.
\item Show that the function from 4 is measurable.
\item Show that for any $n \in \bbN$ $X_\# \bbP (\{n\}) = f_\lambda(n)$.
\end{enumerate}

\end{problem}

\begin{problem}\label{prb:discrete_random_variable}
Let $X$ be a random variable that only takes values in $\bbZ$, i.e. $X(\omega) \in \bbZ$ for all $\omega$. Show that there exists a sequence $(p_j)_{j \in \bbZ}$ with $0 \le p_j \le 1$ and $\sum_{j \in \bbZ} p_j = 1$ such that for any Borel measurable set $A$
\[
	\bbP(X \in A) = \sum_{j \in \bbZ} p_n \delta_n(A).
\]
\end{problem}

\begin{problem}\label{prb:expectation_continuous_random_variable}
The goal of this problem is to prove Lemma~\ref{lem:expectation_continuous_random_variable}. 

Consider the set function
\[
	\nu\colon \cB_{\bbR} \to [0,+\infty],\qquad\nu(A) := \int_A \varrho\, \dd \lambda,
\]
which is a measure on the Borel \sigalg/ by Problem~\ref{prb:measure}.
\begin{enumerate}[label=(\alph*)]
\item Prove that $\nu = X_\# \bbP$.
\item Let $g : \bbR \to \bbR$ be a simple function. Show that
\[
	\int_\bbR g \, \dd \nu = \int_\bbR g \rho \, \dd \lambda.
\]
\end{enumerate}

Now consider the general case, with $h : \bbR \to \bbR$ a measurable function such that $h \rho$ is Lebesgue integrable. Consider now the approximation of $h$ by the simple functions $([h]_n)_{n \ge 1}$ defined in Section~\ref{sec:simple-approximation}.
\begin{enumerate}[label=(\alph*)]
\setcounter{enumi}{2}
\item Prove that 
\[
	\int_\bbR h \, \dd \nu = \int_\bbR h \rho \, \dd \lambda.
\]
[Hint: use monotone convergence]
\item Prove Lemma~\ref{lem:expectation_continuous_random_variable}.
\end{enumerate}
\end{problem}

\begin{problem}[Chebyshev's inequality]
	Prove the following statement: 
	
	\smallskip
	Let $(\Omega,\cF,\mu)$ be a measure space and $f\colon \Omega\to \overline{\bbR}$ be an $(\cF,\overline\cB)$-measurable function. Then for any real number $t>0$ and $p\in(0,+\infty)$,
	\[
		\mu\bigl(\{\omega\in\Omega : |f(\omega)|\ge t\}\bigr) \le \frac{1}{t^p}\int_\Omega |f|^p\,\dd\mu.
	\]
\end{problem}
