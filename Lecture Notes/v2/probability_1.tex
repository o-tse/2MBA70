
\section{Random variables and general stochastic objects}\label{sec:random_variables}

% Random variables   

\XXX{ph}{ph}{TODO: write short intro}

\subsection{Definition}

In the course Probability and Modeling two types of random variables were defined: discrete and continuous. Recall that a random variable was defined as a function $X : \Omega \to \bbR$ for some probability space $(\Omega, \cF, \bbP)$ such that
\[
	\{\omega \in \Omega \, : \, X(\omega) \le x\} \in \cF \quad \text{for all } x \in \bbR.
\]
Let us make two observations here. The first is that the set above is simply the preimage $X^{-1}((-\infty,x])$. Secondly, the sets $(-\infty, x]$ generate the Borel \sigalg/. Thus it follows from Lemma~\ref{lem:measurable_condition_generator} that $X$ is a measurable function. This is actual the proper way to define a random variable.

\begin{definition}[Random variable]
A \emph{random variable} is a measurable function from some probability space $(\Omega,\cF, \bbP)$ to the Borel space $(\bbR, \cB_{\bbR})$.
\end{definition}

It is important to observe that the definition of a random variable does not make any specific claims on what the probability space should be. 

Let $X$ be a random variable and recall that its \emph{cumulative distribution function} $F_X : \bbR \to [0,1]$ is defined as
\[
	F_X(t) = \bbP(X \le t).
\] 
The fact that we use $\bbP$ here, which is the probability measure on the space $(\Omega, \cF)$ is not a coincidence.

The idea behind the cdf $F_X(t)$ is that it denotes the "probability" that $X \in (-\infty ,t]$. From the perspective of measure theory, this means we need to assign a measure to the preimage of $(-\infty, t]$ under the measurable function $X$. For this, the only things we have at our disposal are the probability measure $\bbP$ and the measurable function $X$. Now recall from Proposition~\ref{prop:push_forward_measure} that we can always construct a measure from this: the \emph{push-forward measure}. That is exactly what the cumulative distribution is,
\[
	F_X(t) := X_\# \bbP((-\infty, t]) = \bbP(X^{-1}((-\infty,t])).
\]

In fact we can actually define, at a much more general level, random elements in any measurable space and put an associated probability measure on this space by a push-forward.

\begin{definition}[Random elements]
Let $(\Omega,\cF, \bbP)$ be a probability space and $(E,\cG)$ some measurable space. A \emph{random element} in $(E,\cG)$ is a measurable map $X : \Omega \to E$. It associated \emph{probability measure} is defined as the push forward $X_\# \bbP$ of $\bbP$ under $X$, i.e.
\[
	\bbP(X \in A) := \bbP(X^{-1}(A)) \quad \text{for every } A \in \cG.
\]
\end{definition}

Sometimes we use the term \emph{stochastic} instead of \emph{random}. 

With this general definition we can now easily define random vectors, random matrices, random functions and so one. The only thing we need is to start with the appropriate space (vectors, matrices, functions) and turn it into a measurable space by endowing it with a suitable \sigalg/. Here are some examples:

\begin{example}[Random elements]
\hfill
\begin{enumerate}[label=(\alph*)]
\item A random vector in $\bbR^d$ is a random element in $(\bbR^d, \cB_{\bbR^d})$.
\item A random $n \times m$ matrix is a random element in $(\bbR^n \times \bbR^m, \cB_{\bbR^n} \otimes \cB_{\bbR^m})$.
\end{enumerate}
\end{example}

While these are somewhat straightforward examples, there are also more involved ones that are important in probability theory.

\begin{example}[Stochastic processes]
Let $(\Omega,\cF,\bbP)$ be a probability space, $(S,\cS)$ a measurable space and $T$ some index set. Then we denote by $S^T$ the set of all functions $f : T \to S$. For any $t \in T$, denote by $\pi_t : S^T \to S$ the \emph{evaluation function} $\pi_t(f) = f(t)$. Then we endow the space $S^T$ with the \sigalg/ $\cS^T := \sigma(\pi_t \, : \, t\in T)$. A \emph{stochastic process} on $(S,\cS)$ is then defined as a measurable function $X : (\Omega,\cF) \to (S^T, \cS^T)$.

The space $(S,\cS)$ is often called the \emph{state space} of the stochastic process. Often, the index set is taken to be $\bbN$ or $\bbR_{\ge 0}$. However, the construction above allows for more exotic index sets (although this might impact the properties of the associated stochastic processes).
\end{example}

\subsection{Constructing random variables}

Now that we know what random variables are, there is one problem. In order to define an random variable we need to formally define a probability space $(\Omega, \cF, \bbP)$ and measurable function $X: \Omega \to \bbR$. This is different from how we are used to work with random variables. Here we simply present a cdf $F$ and say that $X$ is a random variable with $\bbP(X \le t) = F(t)$, without worrying about a probability space or the measurability if $X$ as a function. It turns out that this way of working with random variables is valid, as for any cdf $F$ we can construct a probability space $(\Omega, \cF, \bbP)$ and measurable function $X$ such that $X_\# \bbP = F$. We will start this construction for a specific random variable and then use it to construct a random variable with any cumulative distribution function.

One of the first random variables you encounter in any course in probability theory is the \emph{standard uniform random variable}. This is a random variable $U$ that takes values in $[0,1]$ such that its cdf satisfies $F(t) = t$ for all $0\le t \le 1$. In the course Probability and Modeling this description would be enough to work with. But now that we know what a random variable actually is, we need a bit more. More precisely, we have to construct a probability space $(\Omega,\cF, \bbP)$ and a measurable function $U : \Omega \to \bbR$ such that 
\begin{equation}\label{eq:cdf_uniform_rv}
	\bbP\left(U^{-1}((-\infty,t])\right) = \begin{cases}
		0 &\text{if } t < 0,\\
		t &\text{if } 0 \le t\le 1,\\
		1 &\text{if } t > 1.
	\end{cases}
\end{equation}

The following result shows that this is indeed possible. Moreover, in its proof we see a first nice usage of the Lebesgue measure.

\begin{proposition}[Uniform random variable]\label{prop:uniform_random_variable}
There exist a probability space $(\Omega,\cF, \bbP)$ and random variable $U$, such that $\bbP\left(U^{-1}((-\infty,t])\right)$ satisfies~\eqref{eq:cdf_uniform_rv}.
\end{proposition}

\begin{proof}
Consider the space $\Omega = [0,1]$ together with the restricted Borel \sigalg/ $\cF = \cB_{[0,1]}$ and as probability measure the restricted Lebesgue measure $\bbP := \lambda|_{[0,1]}$. Now consider the function $U(t) = \mathbf{1}_{(0,1]} \, t$. Then, it follows that
\[
	U^{-1}((-\infty,t]) = \begin{cases}
		\emptyset &\text{if } t \le 0,\\
		(0,t] &\text{if } 0 < t \le 1, \\
		[0,1] &\text{if } t > 1.
	\end{cases}
\]
Since by Theorem~\ref{thm:lebesgue_measure}
\[
	\lambda|_{[0,1]}((0,t]) = \lambda((0,t]) = t,
\]
for any $0 < t \le 1$ we have
\[
	\bbP\left(U^{-1}((-\infty,t])\right) := \lambda|_{[0,1]}\left(U^{-1}((-\infty,t])\right)
	= \begin{cases}
		0 &\text{if } t < 0,\\
		t &\text{if } 0 \le t\le 1,\\
		1 &\text{if } t > 1.
	\end{cases}\qedhere
\]
\end{proof}

The standard uniform random variable is extremely important, as it is the base from which we can construct any other random variable. To illustrate this let us first consider the case of an \emph{exponential random variable} with rate $\lambda > 0$. This is a random variable $X$ with cdf
\[
	F_X(t) = \begin{cases}
		0 &\text{if } t \le 0,\\
		1-e^{-\lambda t} &\text{if } t > 0.
	\end{cases}
\]

For $u \in (0,1)$, write $H(u) := F_X^{-1}(u)$ and note that
\[
	H(u) = \frac{1}{\lambda} \log\left(\frac{1}{1-u}\right).
\]
Now let $U$ be a standard uniform normal random variable and consider the composition $H \circ U : [0,1] \to \bbR$. First, we note that since cdf $F_X(x)$ is strictly monotonic increase, so is $H$. In particular, it follows that for any $t > 0$,
\[
	H^{-1}((-\infty,t]) = (-\infty, H^{-1}(t)] = (-\infty, F_X(t)].
\]
While $H^{-1}((-\infty,t]) = \emptyset$ if $t \le 0$.

Hence we get
\begin{align*}
	(H \circ U)^{-1}((-\infty, t]) = U^{-1}(H^{-1}((-\infty, t]))
	&= \begin{cases}
		U^{-1}(\emptyset) &\text{if } t \le 0,\\
		U^{-1}((-\infty, F_X(t)]) &\text{if } t > 0.
	\end{cases}
\end{align*}
From this it follows that 
\[
	\bbP\left((H \circ U)^{-1}((-\infty, t])\right) = \begin{cases}
			0 &\text{if } t \le 0,\\
			1-e^{-\lambda t} &\text{if } t > 0,
		\end{cases}
\]
from which we conclude that $H \circ U$ is a way to construct an exponential random variable with rate $\lambda$.

The main point of the construction above is to consider the inverse of the cdf $F^{-1}$ and evaluate this on a standard uniform random variable. However, when extending this to the more general case we have to deal with the fact that not every cdf has an inverse. For example, consider a Bernoulli random variable with success probability $0 < p < 1$. Then
\[
	F(t) = \begin{cases}
		0 &\text{if } t < 0, \\
		1-p &\text{if } 0 \le t < 1, \\
		1 &\text{if } t \ge 1,
	\end{cases}
\] 
which does not have an inverse as for any $y \in (0,1-p)$ there is no $t$ such that $F(t) = y$.

Nevertheless, it does hold that any cdf $F$ is non-decreasing and right continuous. For these types of functions, there exists the notion of a \emph{generalized inverse}, defined as
\begin{equation}
	\overleftarrow{F}(u) := \inf\{ x \in \bbR \, : \, F(x) \ge y\}. 
\end{equation}
The construction we used for the exponential random variable can now be generalized by using $\overleftarrow{F}$ instead of $F^{-1}$. This results in the following theorem on the existence of random variables with a given cdf.

\begin{theorem}[Constructing random variables]\label{thm:construction_random_variable}
Let $F : \bbR \to [0,1]$ be a right continuous, non-decreasing function with 
\[
	\lim_{x \to -\infty} F(x) = 0 \quad \text{and} \quad \lim_{x \to \infty} F(x) = 1.
\]
Then there exists a probability space  $(\Omega,\cF, \bbP)$ and random variable $X$, such that 
\[
	\bbP\left(X \in (-\infty,t]\right) := \bbP\left(X^{-1}((-\infty,t])\right) = F(t).
\]
In other words, $X$ is a random variable with cdf $F$.

Moreover, $(\Omega,\cF, \bbP)$ can be chosen as $([0,1], \cB_{[0,1]}, \lambda|_{[0,1]})$ and $X = \overleftarrow{F}\circ U$, where $U$ is the standard uniform random variable.
\end{theorem}

\begin{proof}
We start with the following important observation: 
\[
	\overleftarrow{F}(u) \le x \iff F(x) \ge u.
\]
The implication from right to left is by definition of $\overleftarrow{F}$ and the fact that $F$ is non-decreasing. The implication from left to right is because $F$ is right continuous.


Now let $(\Omega,\cF, \bbP)$ be a probability space and $U$ a standard normal random variable. We will show that $X = \overleftarrow{F} \circ U$ is a random variable with the right probability measure. Since we can construct a standard uniform random variable on the probability $([0,1], \cB_{[0,1]}, \lambda|_{[0,1]})$ this also implies the last part. 

Consider the preimage of $(-\infty, t]$ under $X$. Then, using the above observation, we have
\begin{align*}
	X^{-1}((-\infty,t]) &= \{\omega \in \Omega \, : \, \overleftarrow{F}(U(\omega)) \in (-\infty,t]\}\\
	&= \{\omega \in \Omega \, :\ , U(\omega) \in (-\infty, F(t)]\} = U^{-1}((-\infty, F(t)]) \in \cB_{[0,1]}.
\end{align*}
Hence, $X$ is measurable. Finally, the above computation, together with Proposition~\ref{prop:uniform_random_variable}, also implies that
\[
	\bbP\left(X^{-1}((-\infty,t])\right) = \bbP\left(U^{-1}((-\infty, F(t)])\right) = F(t),
\]
which finished the proof.
\end{proof}

We end this section with an important remark for working with random variables, and random objects in general.

\begin{remark}[Probability spaces are implicit!]
It is important to note that even though we used a very explicit probability space to construct a standard uniform random variable and the random variable $X$ in the proof of Theorem~\ref{thm:construction_random_variable}, in general the probability space will often be \emph{implicit}. That is, if we say that $X$ is a random variable, we assume there is some probability space $(\Omega,\cF, \bbP)$ that makes $X$ into a measurable function with the right cdf. Theorem~\ref{thm:construction_random_variable} actually says that this is okay as we can always construct an appropriate probability space and measurable function to achieve the needed cdf.

Actually, when considering general random objects in $(E, \cG)$ we often also do not explicitly state or define the probability space. Since the relevant measure is defined through the push-forward we often only have to worry about taking the right measurable space $(E, \cG)$.

There are, however, some cases where one should be cautious about the probability space that is used. For example when considering the notion of \emph{convergence in probability} or \emph{almost sure convergence}. Or when constructing joint distributions of random variables. 
\end{remark}

\section{Discrete and continuous random variables}

Most basic courses in probability theory make a distinction between two classes of random variables: \emph{discrete} and \emph{continuous}. In this section we will put these concepts into the framework of measure theory and provide definitions for the probability mass function and the probability density function.

\subsection{Discrete random variables}
We say that a random variable $X$ is discrete if there exists a countable (possibly infinite) set $N \subset \bbR$ such that $X(\omega) \in N$ for all $\omega \in \Omega$. 

In practice, and throughout these lecture notes, we only consider discrete random variables $X$ whose outcomes are in $\bbZ$. In Problem~\ref{prb:discrete_rv_on_Z} you can show that this can be done without loss of generality. 

A key concept for discrete random variables is the \emph{probability mass function} (pmf), which is often defined as $f(j) = \bbP(X = j)$ for $j \in \bbZ$. The next result shows that any discrete random variable has a pmf.

\begin{lemma}[Probability mass function]\label{lem:existence_pmf}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space and $X$ a discrete random variable, i.e. $X(\omega) \in \bbZ$ for all $\omega \in \Omega$. Then there exists a sequence $(p_j)_{j \in \bbZ}$ with $\sum_{j \in \bbZ} p_j = 1$ that for any measurable set $A$,
\[
	\bbP(X \in A) = \sum_{j \in \bbZ} \delta_j(A) p_j.
\]
\end{lemma}

\begin{proof}
See Problem~\ref{prb:discrete_rv_has_pmf}.
\end{proof}

With this result, we can now formally define the \emph{probability mass function} (pmf) of $X$ as a function $f : \bbZ \to [0,1]$ given by $f(j) = p_j$.

Note that with this definition we indeed have
\[
	F(t) = \sum_{j = -\infty}^t p_j
\]
and that
\[
	f(j) = \bbP(X = j).
\]

Below are some examples of some classical discrete random variables.

\begin{example}\hfill
\begin{enumerate}[label=(\alph*)]
\item \textit{Bernoulli}: Let $p \in [0,1]$. A random variable $X : \Omega \to \{0,1\}$ with $\bbP(X = 1) = p$ is called a \emph{Bernoulli random variable} with success probability $p$.
\item \textit{Binomial}: Let $n \in \mathbb{N}$ and $p \in [0,1]$. A random variable $X : \Omega \to \mathbb{N}_0$ with $\bbP(X = j) = \binom{n}{j} p^j (1-p)^{n-j}$ is called a \emph{Binomial random variable} with $n$ trials and success probability $p$
\item \textit{Poisson}: Let $\lambda > 0$. A random variable $X : \Omega \to \mathbb{N}_0$ with $\bbP(X = j) = \frac{\lambda^j}{j!} e^{-\lambda}$ is called a \emph{Poisson random variable} with mean $\lambda$.
\end{enumerate}
\end{example}

\subsection{Continuous random variables}

In contrast to discrete random variables, we say a random variable $X$ is \emph{continuous} if there exist a family  $(\mathcal{I}_i)_{i \in I}$ of pairwise disjoint intervals such that $\mathrm{im}(X) = \bigcup_{i \in I} \mathcal{I}_i$.
A concept for continuous random variables related to that of the probability mass function is the probability density function. 

\begin{definition}[Probability density function]\label{def:pdf}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space and $X$ a continuous random variable. We say that $X$ has a \emph{probability density function} (pdf) $\rho : \bbR \to [0,\infty)$, if for every $t \in \bbR$,
\[
	X_\# \bbP((-\infty , t]) = \int_{(-\infty,t]} \rho \, \dd \lambda.
\]
In particular, a probability density function must be integrable.
\end{definition}

Note that unlike a probability mass function, a probability density function can yield values larger than $1$. An example of this is the pdf for a continuous uniform random variable on $[0,1/2]$, which is given by $\rho(x) = 2 \mathbf{1}_{x \in [0,1/2]}$.

Technically, the notion of a probability density can be defined for any random variable $X$. However, unlike the pmf for discrete random variables, not all random variables, including continuous ones, have a pdf (see Problem~\ref{prb:no_pdf}).

The following classical examples of continuous random variables do all have a pdf.

\begin{example}\hfill
\begin{enumerate}[label=(\alph*)]
\item \textit{Exponential}: Let $\lambda > 0$. A random variable $X : \Omega \to \bbR_+$ with pdf $\rho(x) = \lambda e^{-\lambda x}$ is called an \emph{Exponential random variable} with rate $\lambda$.
\item \textit{Normal}: Let $\mu \in \bbR$ and $\sigma > 0$. A random variable $X : \Omega \to \bbR_+$ with pdf $\rho(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-(x - \mu)^2/(2\sigma^2)}$ is called a \emph{Normal random variable} with mean $\mu$ and variance $\sigma^2$.
\item \textit{Pareto}: Let $\xi, \alpha > 0$. A random variable $X : \Omega \to [\xi, \infty)$ with pdf $\rho(x) = \frac{\alpha \xi^\alpha}{x^{\alpha +1}}$ is called a \emph{Pareto random variable} with scale $\xi$ and shape $\alpha$.
\end{enumerate} 
\end{example}

\section{Multi-variate random variables}

[TODO]

\section{Expected value of random variables}

One of the first things you learn to compute for a random variable is its \emph{expected value}, often denoted as $\bbE[X]$. Armed with the definition of random variables and the notion of integration we can formally define what the expected value of a random variable is.

\begin{definition}[Expected value]\label{def:expectation_random_variable}
Let $(\Omega. \cF, \bbP)$ be a probability space and $X$ and random variable. Then
\[
	\bbE[X] := \int_\Omega X \, \dd \bbP.
\]
\end{definition}



In the course Probability and Modeling you have seen the following definition of the expected value of $h(X)$, where $h$ is a function and $X$ a discrete random variable:
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} j \bbP(h(X) = j).
\]
The following result, called the law of the unconscious statistician, expressed this in terms of the probabilities pmf of $X$:
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} h(j) p(j).
\] 

We will now use the change of variables proposition to prove this result, given the general definition for the expected value in Definition~\ref{def:expectation_random_variable}.


\begin{lemma}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space, $X$ be a discrete random variable and consider a function $h : \bbR \to \bbR$ such that $h\circ X$ is $\bbP$-integrable. Then
\[
	\bbE[h(X)] = \sum_{j \in \bbZ} h(j) p(j),
\] 
where $p$ is the pmf of $X$.
\end{lemma}

\begin{proof}
Recall the definition of the positive and negative part of a measurable function $f$, denoted by respectively $f^+$ and $f^-$. Further, recall that 
\[
	\int_\Omega f \, \dd \mu := \int_\Omega f^+ \, \dd \mu - \int_\Omega f^- \, \dd \mu 
\] 
Now, for any $n \in \bbN$ define the functions
\[
	g_n^\pm = \sum_{j = -n}^n (h \circ X)^\pm \mathbf{1}_{X^{-1}(j)}.
\]
Then $g_n^\pm \le g_{n+1}^\pm$ and
\[
	\lim_{n \to \infty} g_n^\pm = (h \circ X)^\pm.
\]
Then, using the monotone convergence theorem we get
\begin{align*}
	\int_\Omega (h \circ X)^\pm \, \dd \bbP &= \int_\Omega \lim_{n \to \infty} g_n^\pm \, \dd \bbP\\
	&= \lim_{n \to \infty} \int_\Omega g_n^\pm \, \dd \bbP \\
	&= \lim_{n \to \infty} \sum_{j = -n}^n \int_\Omega (h \circ X)^\pm \mathbf{1}_{X^{-1}(j)} \, \dd \bbP\\
	&= \lim_{n \to \infty} \sum_{j = -n}^n \int_{X^{-1}(j)} h^\pm(j) \, \dd \bbP\\
	&= \lim_{n \to \infty} \sum_{j = -n}^n h^\pm(j) \bbP(X^{-1}(j)) \\
	&= \lim_{n \to \infty} \sum_{j = -n}^n h^\pm(j) p(j) = \sum_{j \in \bbZ} h^\pm(j) p(j).
\end{align*}
Since $h\circ X$ is $\bbP$-integrable if and only if it positive and negative part are, we conclude that
\[
	\int_\Omega (h \circ X) \, \dd \bbP = \int_\Omega (h \circ X)^+ \, \dd \bbP
	- \int_\Omega (h \circ X)^- \, \dd \bbP = \sum_{j \in \bbZ} h(j) p(j).
\]
\end{proof}

Let us now turn to the other class of random variables: continuous random variables. Here we introduced the notion of a \emph{probability density function} $\rho$ so that $F(t)$ was equal to the integral of $\rho$ on $(-\infty,t]$. 

Now recall that in the case of a continuous random variable $Y$ with a probability density $\rho$, there was also a formula for the expected value,
\[
	\bbE[h(Y)] = \int_\bbR h(x) \rho(x) \, \dd x.
\]
Again, this formula is correct (under some mild obvious conditions) and follows from Definition~\ref{def:expectation_random_variable} after applying a change of variables. The proof of this result is left as an exercise.

\begin{lemma}\label{lem:expectation_continuous_random_variable}
Let $(\Omega, \cF, \mathbb{P})$ be a probability space, $X$ a continuous random variable with probability density $\rho$ and let $h : \bbR \to \bbR$ be a measurable function such that $h \rho$ is Lebesgue integrable. Then
\[
	\bbE[h(Y)] = \int_\bbR h \rho \, \dd \lambda.
\]
\end{lemma}

\begin{proof}
See problem~\ref{prb:expectation_continuous_random_variable}
\end{proof}

\section{The Markov inequality}

The following result states the Markov inequality. The trick used in the proof can be used to obtain many similar inequalities.

\begin{lemma}[The Markov inequality]
	Let $(\Omega, \cF, \mu)$ be a measurable space and let $f$ be a $\mu$-integrable function. For any $t>0$,
	\[
	\mu\bigl(\{ \omega \in \Omega : \ |f(\omega)| \geq t \}\bigr) \leq \frac{1}{t} \int_\Omega |f| \,\dd \mu.
	\]
\end{lemma}
\begin{proof} The result follows easily from
	\[
		\int_\Omega |f|\,\dd \mu \ge \int_{\{|f|\ge t\}} |f|\,\dd \mu \ge t\,\mu\bigl(\{|f|\ge t\}\bigr)  \qedhere
	\]
\end{proof}


In probability language, the Markov inequality looks as follows.

\begin{lemma}[The Markov inequality]
	Let $(\Omega, \cF, \mathbb{P})$ be a probability space and let $X$ be a random variable. For any $t>0$,
	\[
	\mathbb{P}(|X| \geq t) \leq \frac{1}{t} \mathbb{E}[|X|].
	\]
\end{lemma}

\section{Problems}

\begin{problem}
Let $X,Y$ be two random variables with cdfs $F_X$ and $F_Y$, respectively. 
\begin{enumerate}[label=(\alph*)]
\item Prove that if $F_X(t) = F_Y(t)$ for every $t \in \bbR$, then $X_\# \bbP = Y_\# \bbP$. 
\end{enumerate}
The above relation is often denoted as $X \stackrel{d}{=} Y$ ($X$ is equal to $Y$ in distribution). Basically, this definition says that two random variables are considered equal if their distribution functions are the same, which is implied by the equality of their cdfs. However, the use of the equality sign can be slightly misleading.
\begin{enumerate}
\setcounter{enumi}{1}
\item Let $X$ be a random variable. Construct a random variable $Y$ such that $X_\# \bbP = Y_\# \bbP$, but $X \ne Y$ as functions $\Omega \to \bbR$.
\end{enumerate}
\end{problem}

\begin{problem}
In this problem we will	explicitly construct two random variables $X$ and $Y$ such that $X$ is \emph{Poisson} distributed with parameter $\lambda>0$, and $Y$ is \emph{Cauchy} distributed with parameter $\gamma>0$. 

For this we define the Poisson probability mass function (pmf)
\begin{equation}
	f_\lambda(n) := \frac{e^{-\lambda}\lambda^n}{n!},\qquad \forall\,n\in\bbN,
\end{equation}
and the Cauchy cumulative distribution function (cdf)
\begin{equation}
	H_\gamma(z) := \frac{1}{\pi}\arctan\left(\frac{z}{\gamma}\right) + \frac{1}{2},\qquad \forall\,z\in\bbR.
\end{equation}

We will first construct the Cauchy random variable.
\begin{enumerate}[label=(\alph*)]
\item Define an explicit probability space $(\Omega, \cF, \bbP)$ and provide an explicit formula for the function $Y\colon \Omega\to\bbR$ such that $Y_\# \bbP = H_\gamma$.
\item Show that the function from 1 is measurable.
\end{enumerate}

For the Poisson random variable we first need to go from the pmf to its cdf.
\begin{enumerate}[label=(\alph*)]
\setcounter{enumi}{2}
\item Express the cumulative distribution function $F_\lambda$ for a Poisson random variable in terms of the pmf $f_\lambda$.
\item Define an explicit probability space $(\Omega, \cF, \bbP)$ and provide a formula for the function $X\colon \Omega\to\bbR$ such that $X_\# \bbP = F_\lambda$.
\item Show that the function from 4 is measurable.
\item Show that for any $n \in \bbN$ $X_\# \bbP (\{n\}) = f_\lambda(n)$.
\end{enumerate}

\end{problem}

\begin{problem}\label{prb:discrete_rv_on_Z}
Here you will show that without loss of generality we can consider discrete random variables as measurable functions $X: \Omega \to \bbZ$.

\begin{enumerate}[label=(\alph*)]
\item Let $N_1 \subset [0,\infty)$ be a countable set. Construct a subset $Z_1 \subseteq \bbZ_{\ge 0}$ and a bijection $\phi_1 : N_1 \to \bbZ_{\ge 0}$.
\item Let $N_2 \subset (-\infty,0)$ be a countable set. Construct a subset $Z_2 \subseteq \bbZ_{< 0}$ and a bijection $\phi_2 : N_2 \to \bbZ_{< 0}$.
\item Conclude that for any countable set $N \subset \bbR$ there exists a $Z \subseteq \bbZ$ and a bijection $\phi : N \to Z$. 
\item Prove that this bijection is measurable.
\item Now let $Y : \Omega \to \bbZ$ be defined as $Y(\omega) = \phi(X(\omega))$. Show that $Y$ is a discrete random variable and that its cdf completely determines the cdf of $X$.
\end{enumerate}
\end{problem}

\begin{problem}\label{prb:discrete_rv_has_pmf}
Prove Lemma~\ref{lem:existence_pmf}
\end{problem}

\begin{problem}\label{prb:no_pdf}
In this exercise we will see that not all random variables have probability density functions. We start with a very simple case.
\begin{enumerate}[label=(\alph*)]
\item Let $X : \Omega \to \bbZ$ be a discrete random variable. Prove that $X$ does not have a pdf.
\end{enumerate}

You might be inclined to think that the problem with existence of pdfs lies with the discrete nature. Suppose the following two properties are satisfied:
\begin{enumerate}
\item There exists a uncountable set $C \subset [0,1]$ that has Lebesgue measure zero,
\item There exists a function continuous increasing function $F: [0,1] \to [0,1]$ that is constant outside $C$, $f(c) = 0$ for all $c \in C$, $F(0) = 0$ and $F(1) = 1$.
\end{enumerate}

\begin{enumerate}[label=(\alph*)]
\setcounter{enumi}{1}
\item Let $X : \Omega \to [0,1]$ be a random variable with cdf given by the function $F$ described above. Show that $X$ does not have a pdf.
\end{enumerate}

The setting we described above does occur and is a classical example for a continuous random variable in $\bbR$ that does not have a pdf. The set $C$ is called the Cantor set and the function $F$ is constructed using the so-called the Cantor function. 

Simple example of continuous stochastic objects without pdfs exist for higher dimensions, for example $\bbR^2$. And we will come back to this in a later chapter [REF].
\end{problem}

\begin{problem}\label{prb:expectation_continuous_random_variable}
The goal of this problem is to prove Lemma~\ref{lem:expectation_continuous_random_variable}. 

Consider the set function
\[
	\nu\colon \cB_{\bbR} \to [0,+\infty],\qquad\nu(A) := \int_A \varrho\, \dd \lambda,
\]
which is a measure on the Borel \sigalg/ by Problem~\ref{prb:measure}.
\begin{enumerate}[label=(\alph*)]
\item Prove that $\nu = X_\# \bbP$.
\item Let $g : \bbR \to \bbR$ be a simple function. Show that
\[
	\int_\bbR g \, \dd \nu = \int_\bbR g \rho \, \dd \lambda.
\]
\end{enumerate}

Now consider the general case, with $h : \bbR \to \bbR$ a measurable function such that $h \rho$ is Lebesgue integrable. Consider now the approximation of $h$ by the simple functions $([h]_n)_{n \ge 1}$ defined in Section~\ref{sec:simple-approximation}.
\begin{enumerate}[label=(\alph*)]
\setcounter{enumi}{2}
\item Prove that 
\[
	\int_\bbR h \, \dd \nu = \int_\bbR h \rho \, \dd \lambda.
\]
[Hint: use monotone convergence]
\item Prove Lemma~\ref{lem:expectation_continuous_random_variable}.
\end{enumerate}
\end{problem}

\begin{problem}[Chebyshev's inequality]
	Prove the following statement: 
	
	\smallskip
	Let $(\Omega,\cF,\mu)$ be a measure space and $f\colon \Omega\to \overline{\bbR}$ be an $(\cF,\overline\cB)$-measurable function. Then for any real number $t>0$ and $p\in(0,+\infty)$,
	\[
		\mu\bigl(\{\omega\in\Omega : |f(\omega)|\ge t\}\bigr) \le \frac{1}{t^p}\int_\Omega |f|^p\,\dd\mu.
	\]
\end{problem}
